        -:    0:Source:slabs.c
        -:    0:Graph:slabs.gcno
        -:    0:Data:slabs.gcda
        -:    0:Runs:167
        -:    0:Programs:1
        -:    1:/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */
        -:    2:/*
        -:    3: * Slabs memory allocation, based on powers-of-N. Slabs are up to 1MB in size
        -:    4: * and are divided into chunks. The chunk sizes start off at the size of the
        -:    5: * "item" structure plus space for a small key and value. They increase by
        -:    6: * a multiplier factor from there, up to half the maximum slab size. The last
        -:    7: * slab size is always 1MB, since that's the maximum item size allowed by the
        -:    8: * memcached protocol.
        -:    9: */
        -:   10:#include "memcached.h"
        -:   11:#include <sys/stat.h>
        -:   12:#include <sys/socket.h>
        -:   13:#include <sys/signal.h>
        -:   14:#include <sys/resource.h>
        -:   15:#include <fcntl.h>
        -:   16:#include <netinet/in.h>
        -:   17:#include <errno.h>
        -:   18:#include <stdlib.h>
        -:   19:#include <stdio.h>
        -:   20:#include <string.h>
        -:   21:#include <assert.h>
        -:   22:#include <pthread.h>
        -:   23:
        -:   24://#define DEBUG_SLAB_MOVER
        -:   25:/* powers-of-N allocation structures */
        -:   26:
        -:   27:typedef struct {
        -:   28:    unsigned int size;      /* sizes of items */
        -:   29:    unsigned int perslab;   /* how many items per slab */
        -:   30:
        -:   31:    void *slots;           /* list of item ptrs */
        -:   32:    unsigned int sl_curr;   /* total free items in list */
        -:   33:
        -:   34:    unsigned int slabs;     /* how many slabs were allocated for this class */
        -:   35:
        -:   36:    void **slab_list;       /* array of slab pointers */
        -:   37:    unsigned int list_size; /* size of prev array */
        -:   38:
        -:   39:    size_t requested; /* The number of requested bytes */
        -:   40:} slabclass_t;
        -:   41:
        -:   42:static slabclass_t slabclass[MAX_NUMBER_OF_SLAB_CLASSES];
        -:   43:static size_t mem_limit = 0;
        -:   44:static size_t mem_malloced = 0;
        -:   45:/* If the memory limit has been hit once. Used as a hint to decide when to
        -:   46: * early-wake the LRU maintenance thread */
        -:   47:static bool mem_limit_reached = false;
        -:   48:static int power_largest;
        -:   49:
        -:   50:static void *mem_base = NULL;
        -:   51:static void *mem_current = NULL;
        -:   52:static size_t mem_avail = 0;
        -:   53:
        -:   54:/**
        -:   55: * Access to the slab allocator is protected by this lock
        -:   56: */
        -:   57:static pthread_mutex_t slabs_lock = PTHREAD_MUTEX_INITIALIZER;
        -:   58:static pthread_mutex_t slabs_rebalance_lock = PTHREAD_MUTEX_INITIALIZER;
        -:   59:
        -:   60:/*
        -:   61: * Forward Declarations
        -:   62: */
        -:   63:static int do_slabs_newslab(const unsigned int id);
        -:   64:static void *memory_allocate(size_t size);
        -:   65:static void do_slabs_free(void *ptr, const size_t size, unsigned int id);
        -:   66:
        -:   67:/* Preallocate as many slab pages as possible (called from slabs_init)
        -:   68:   on start-up, so users don't get confused out-of-memory errors when
        -:   69:   they do have free (in-slab) space, but no space to make new slabs.
        -:   70:   if maxslabs is 18 (POWER_LARGEST - POWER_SMALLEST + 1), then all
        -:   71:   slab types can be made.  if max memory is less than 18 MB, only the
        -:   72:   smaller ones will be made.  */
        -:   73:static void slabs_preallocate (const unsigned int maxslabs);
        -:   74:
        -:   75:/*
        -:   76: * Figures out which slab class (chunk size) is required to store an item of
        -:   77: * a given size.
        -:   78: *
        -:   79: * Given object size, return id to use when allocating/freeing memory for object
        -:   80: * 0 means error: can't store such a large object
        -:   81: */
        -:   82:
    53253:   83:unsigned int slabs_clsid(const size_t size) {
    53253:   84:    int res = POWER_SMALLEST;
        -:   85:
    53253:   86:    if (size == 0)
        -:   87:        return 0;
   798447:   88:    while (size > slabclass[res].size)
   745202:   89:        if (res++ == power_largest)     /* won't fit in the biggest slab */
        -:   90:            return 0;
    53245:   91:    return res;
        -:   92:}
        -:   93:
        -:   94:/**
        -:   95: * Determines the chunk sizes and initializes the slab class descriptors
        -:   96: * accordingly.
        -:   97: */
       83:   98:void slabs_init(const size_t limit, const double factor, const bool prealloc) {
       83:   99:    int i = POWER_SMALLEST - 1;
       83:  100:    unsigned int size = sizeof(item) + settings.chunk_size;
        -:  101:
       83:  102:    mem_limit = limit;
        -:  103:
       83:  104:    if (prealloc) {
        -:  105:        /* Allocate everything in a big chunk with malloc */
    #####:  106:        mem_base = malloc(mem_limit);
    #####:  107:        if (mem_base != NULL) {
    #####:  108:            mem_current = mem_base;
    #####:  109:            mem_avail = mem_limit;
        -:  110:        } else {
    #####:  111:            fprintf(stderr, "Warning: Failed to allocate requested memory in"
        -:  112:                    " one large chunk.\nWill allocate in smaller chunks\n");
        -:  113:        }
        -:  114:    }
        -:  115:
        -:  116:    memset(slabclass, 0, sizeof(slabclass));
        -:  117:
     3467:  118:    while (++i < MAX_NUMBER_OF_SLAB_CLASSES-1 && size <= settings.item_size_max / factor) {
        -:  119:        /* Make sure items are always n-byte aligned */
     3384:  120:        if (size % CHUNK_ALIGN_BYTES)
     2311:  121:            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);
        -:  122:
     3384:  123:        slabclass[i].size = size;
     3384:  124:        slabclass[i].perslab = settings.item_size_max / slabclass[i].size;
     3384:  125:        size *= factor;
     3384:  126:        if (settings.verbose > 1) {
      123:  127:            fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
        -:  128:                    i, slabclass[i].size, slabclass[i].perslab);
        -:  129:        }
        -:  130:    }
        -:  131:
       83:  132:    power_largest = i;
       83:  133:    slabclass[power_largest].size = settings.item_size_max;
       83:  134:    slabclass[power_largest].perslab = 1;
       83:  135:    if (settings.verbose > 1) {
        3:  136:        fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
        -:  137:                i, slabclass[i].size, slabclass[i].perslab);
        -:  138:    }
        -:  139:
        -:  140:    /* for the test suite:  faking of how much we've already malloc'd */
        -:  141:    {
       83:  142:        char *t_initial_malloc = getenv("T_MEMD_INITIAL_MALLOC");
       83:  143:        if (t_initial_malloc) {
        1:  144:            mem_malloced = (size_t)atol(t_initial_malloc);
        -:  145:        }
        -:  146:
        -:  147:    }
        -:  148:
       83:  149:    if (prealloc) {
    #####:  150:        slabs_preallocate(power_largest);
        -:  151:    }
       83:  152:}
        -:  153:
    #####:  154:static void slabs_preallocate (const unsigned int maxslabs) {
        -:  155:    int i;
    #####:  156:    unsigned int prealloc = 0;
        -:  157:
        -:  158:    /* pre-allocate a 1MB slab in every size class so people don't get
        -:  159:       confused by non-intuitive "SERVER_ERROR out of memory"
        -:  160:       messages.  this is the most common question on the mailing
        -:  161:       list.  if you really don't want this, you can rebuild without
        -:  162:       these three lines.  */
        -:  163:
    #####:  164:    for (i = POWER_SMALLEST; i < MAX_NUMBER_OF_SLAB_CLASSES; i++) {
    #####:  165:        if (++prealloc > maxslabs)
    #####:  166:            return;
    #####:  167:        if (do_slabs_newslab(i) == 0) {
    #####:  168:            fprintf(stderr, "Error while preallocating slab memory!\n"
        -:  169:                "If using -L or other prealloc options, max memory must be "
        -:  170:                "at least %d megabytes.\n", power_largest);
    #####:  171:            exit(1);
        -:  172:        }
        -:  173:    }
        -:  174:
        -:  175:}
        -:  176:
      613:  177:static int grow_slab_list (const unsigned int id) {
      613:  178:    slabclass_t *p = &slabclass[id];
      613:  179:    if (p->slabs == p->list_size) {
      146:  180:        size_t new_size =  (p->list_size != 0) ? p->list_size * 2 : 16;
      146:  181:        void *new_list = realloc(p->slab_list, new_size * sizeof(void *));
      146:  182:        if (new_list == 0) return 0;
      146:  183:        p->list_size = new_size;
      146:  184:        p->slab_list = new_list;
        -:  185:    }
        -:  186:    return 1;
        -:  187:}
        -:  188:
      535:  189:static void split_slab_page_into_freelist(char *ptr, const unsigned int id) {
      535:  190:    slabclass_t *p = &slabclass[id];
        -:  191:    int x;
   502460:  192:    for (x = 0; x < p->perslab; x++) {
   501925:  193:        do_slabs_free(ptr, 0, id);
   501925:  194:        ptr += p->size;
        -:  195:    }
      535:  196:}
        -:  197:
        -:  198:/* Fast FIFO queue */
        -:  199:static void *get_page_from_global_pool(void) {
      525:  200:    slabclass_t *p = &slabclass[SLAB_GLOBAL_PAGE_POOL];
      525:  201:    if (p->slabs < 1) {
        -:  202:        return NULL;
        -:  203:    }
       78:  204:    char *ret = p->slab_list[p->slabs - 1];
       78:  205:    p->slabs--;
        -:  206:    return ret;
        -:  207:}
        -:  208:
     8051:  209:static int do_slabs_newslab(const unsigned int id) {
     8051:  210:    slabclass_t *p = &slabclass[id];
     8051:  211:    slabclass_t *g = &slabclass[SLAB_GLOBAL_PAGE_POOL];
     9249:  212:    int len = settings.slab_reassign ? settings.item_size_max
     1198:  213:        : p->size * p->perslab;
        -:  214:    char *ptr;
        -:  215:
     8051:  216:    if ((mem_limit && mem_malloced + len > mem_limit && p->slabs > 0
     7589:  217:         && g->slabs == 0)) {
     7526:  218:        mem_limit_reached = true;
        -:  219:        MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);
     7526:  220:        return 0;
        -:  221:    }
        -:  222:
     1050:  223:    if ((grow_slab_list(id) == 0) ||
      447:  224:        (((ptr = get_page_from_global_pool()) == NULL) &&
      447:  225:        ((ptr = memory_allocate((size_t)len)) == 0))) {
        -:  226:
        -:  227:        MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);
        -:  228:        return 0;
        -:  229:    }
        -:  230:
      525:  231:    memset(ptr, 0, (size_t)len);
      525:  232:    split_slab_page_into_freelist(ptr, id);
        -:  233:
      525:  234:    p->slab_list[p->slabs++] = ptr;
        -:  235:    MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id);
        -:  236:
      525:  237:    return 1;
        -:  238:}
        -:  239:
        -:  240:/*@null@*/
    60951:  241:static void *do_slabs_alloc(const size_t size, unsigned int id, unsigned int *total_chunks,
        -:  242:        unsigned int flags) {
        -:  243:    slabclass_t *p;
    60951:  244:    void *ret = NULL;
    60951:  245:    item *it = NULL;
        -:  246:
    60951:  247:    if (id < POWER_SMALLEST || id > power_largest) {
        -:  248:        MEMCACHED_SLABS_ALLOCATE_FAILED(size, 0);
        -:  249:        return NULL;
        -:  250:    }
    60951:  251:    p = &slabclass[id];
    60951:  252:    assert(p->sl_curr == 0 || ((item *)p->slots)->slabs_clsid == 0);
        -:  253:
    60951:  254:    if (total_chunks != NULL) {
    60739:  255:        *total_chunks = p->slabs * p->perslab;
        -:  256:    }
        -:  257:    /* fail unless we have space at the end of a recently allocated page,
        -:  258:       we have something on our freelist, or we could allocate a new page */
    60951:  259:    if (p->sl_curr == 0 && flags != SLABS_ALLOC_NO_NEWPAGE) {
     8051:  260:        do_slabs_newslab(id);
        -:  261:    }
        -:  262:
    60951:  263:    if (p->sl_curr != 0) {
        -:  264:        /* return off our freelist */
    53345:  265:        it = (item *)p->slots;
    53345:  266:        p->slots = it->next;
    53345:  267:        if (it->next) it->next->prev = 0;
        -:  268:        /* Kill flag and initialize refcount here for lock safety in slab
        -:  269:         * mover's freeness detection. */
    53345:  270:        it->it_flags &= ~ITEM_SLABBED;
    53345:  271:        it->refcount = 1;
    53345:  272:        p->sl_curr--;
    53345:  273:        ret = (void *)it;
        -:  274:    } else {
        -:  275:        ret = NULL;
        -:  276:    }
        -:  277:
    60951:  278:    if (ret) {
    53345:  279:        p->requested += size;
        -:  280:        MEMCACHED_SLABS_ALLOCATE(size, id, p->size, ret);
        -:  281:    } else {
        -:  282:        MEMCACHED_SLABS_ALLOCATE_FAILED(size, id);
        -:  283:    }
        -:  284:
    60951:  285:    return ret;
        -:  286:}
        -:  287:
   539550:  288:static void do_slabs_free(void *ptr, const size_t size, unsigned int id) {
        -:  289:    slabclass_t *p;
        -:  290:    item *it;
        -:  291:
   539550:  292:    assert(id >= POWER_SMALLEST && id <= power_largest);
   539550:  293:    if (id < POWER_SMALLEST || id > power_largest)
        -:  294:        return;
        -:  295:
        -:  296:    MEMCACHED_SLABS_FREE(size, id, ptr);
   539550:  297:    p = &slabclass[id];
        -:  298:
   539550:  299:    it = (item *)ptr;
   539550:  300:    it->it_flags = ITEM_SLABBED;
   539550:  301:    it->slabs_clsid = 0;
   539550:  302:    it->prev = 0;
   539550:  303:    it->next = p->slots;
   539550:  304:    if (it->next) it->next->prev = it;
   539550:  305:    p->slots = it;
        -:  306:
   539550:  307:    p->sl_curr++;
   539550:  308:    p->requested -= size;
   539550:  309:    return;
        -:  310:}
        -:  311:
       47:  312:static int nz_strcmp(int nzlength, const char *nz, const char *z) {
       47:  313:    int zlength=strlen(z);
       47:  314:    return (zlength == nzlength) && (strncmp(nz, z, zlength) == 0) ? 0 : -1;
        -:  315:}
        -:  316:
     3863:  317:bool get_stats(const char *stat_type, int nkey, ADD_STAT add_stats, void *c) {
     3863:  318:    bool ret = true;
        -:  319:
     3863:  320:    if (add_stats != NULL) {
     3863:  321:        if (!stat_type) {
        -:  322:            /* prepare general statistics for the engine */
     3833:  323:            STATS_LOCK();
     3833:  324:            APPEND_STAT("bytes", "%llu", (unsigned long long)stats.curr_bytes);
     3833:  325:            APPEND_STAT("curr_items", "%u", stats.curr_items);
     3833:  326:            APPEND_STAT("total_items", "%u", stats.total_items);
     3833:  327:            STATS_UNLOCK();
     3832:  328:            if (settings.slab_automove > 0) {
        5:  329:                pthread_mutex_lock(&slabs_lock);
        5:  330:                APPEND_STAT("slab_global_page_pool", "%u", slabclass[SLAB_GLOBAL_PAGE_POOL].slabs);
        5:  331:                pthread_mutex_unlock(&slabs_lock);
        -:  332:            }
     3832:  333:            item_stats_totals(add_stats, c);
       30:  334:        } else if (nz_strcmp(nkey, stat_type, "items") == 0) {
       13:  335:            item_stats(add_stats, c);
       17:  336:        } else if (nz_strcmp(nkey, stat_type, "slabs") == 0) {
       17:  337:            slabs_stats(add_stats, c);
    #####:  338:        } else if (nz_strcmp(nkey, stat_type, "sizes") == 0) {
    #####:  339:            item_stats_sizes(add_stats, c);
        -:  340:        } else {
        -:  341:            ret = false;
        -:  342:        }
        -:  343:    } else {
        -:  344:        ret = false;
        -:  345:    }
        -:  346:
     3862:  347:    return ret;
        -:  348:}
        -:  349:
        -:  350:/*@null@*/
       17:  351:static void do_slabs_stats(ADD_STAT add_stats, void *c) {
        -:  352:    int i, total;
        -:  353:    /* Get the per-thread stats which contain some interesting aggregates */
        -:  354:    struct thread_stats thread_stats;
       17:  355:    threadlocal_stats_aggregate(&thread_stats);
        -:  356:
       17:  357:    total = 0;
      731:  358:    for(i = POWER_SMALLEST; i <= power_largest; i++) {
      714:  359:        slabclass_t *p = &slabclass[i];
      714:  360:        if (p->slabs != 0) {
        -:  361:            uint32_t perslab, slabs;
       62:  362:            slabs = p->slabs;
       62:  363:            perslab = p->perslab;
        -:  364:
        -:  365:            char key_str[STAT_KEY_LEN];
        -:  366:            char val_str[STAT_VAL_LEN];
       62:  367:            int klen = 0, vlen = 0;
        -:  368:
      124:  369:            APPEND_NUM_STAT(i, "chunk_size", "%u", p->size);
      124:  370:            APPEND_NUM_STAT(i, "chunks_per_page", "%u", perslab);
      124:  371:            APPEND_NUM_STAT(i, "total_pages", "%u", slabs);
      124:  372:            APPEND_NUM_STAT(i, "total_chunks", "%u", slabs * perslab);
      124:  373:            APPEND_NUM_STAT(i, "used_chunks", "%u",
        -:  374:                            slabs*perslab - p->sl_curr);
      124:  375:            APPEND_NUM_STAT(i, "free_chunks", "%u", p->sl_curr);
        -:  376:            /* Stat is dead, but displaying zero instead of removing it. */
      124:  377:            APPEND_NUM_STAT(i, "free_chunks_end", "%u", 0);
      124:  378:            APPEND_NUM_STAT(i, "mem_requested", "%llu",
        -:  379:                            (unsigned long long)p->requested);
      124:  380:            APPEND_NUM_STAT(i, "get_hits", "%llu",
        -:  381:                    (unsigned long long)thread_stats.slab_stats[i].get_hits);
      124:  382:            APPEND_NUM_STAT(i, "cmd_set", "%llu",
        -:  383:                    (unsigned long long)thread_stats.slab_stats[i].set_cmds);
      124:  384:            APPEND_NUM_STAT(i, "delete_hits", "%llu",
        -:  385:                    (unsigned long long)thread_stats.slab_stats[i].delete_hits);
      124:  386:            APPEND_NUM_STAT(i, "incr_hits", "%llu",
        -:  387:                    (unsigned long long)thread_stats.slab_stats[i].incr_hits);
      124:  388:            APPEND_NUM_STAT(i, "decr_hits", "%llu",
        -:  389:                    (unsigned long long)thread_stats.slab_stats[i].decr_hits);
      124:  390:            APPEND_NUM_STAT(i, "cas_hits", "%llu",
        -:  391:                    (unsigned long long)thread_stats.slab_stats[i].cas_hits);
      124:  392:            APPEND_NUM_STAT(i, "cas_badval", "%llu",
        -:  393:                    (unsigned long long)thread_stats.slab_stats[i].cas_badval);
      124:  394:            APPEND_NUM_STAT(i, "touch_hits", "%llu",
        -:  395:                    (unsigned long long)thread_stats.slab_stats[i].touch_hits);
       62:  396:            total++;
        -:  397:        }
        -:  398:    }
        -:  399:
        -:  400:    /* add overall slab stats and append terminator */
        -:  401:
       17:  402:    APPEND_STAT("active_slabs", "%d", total);
       17:  403:    APPEND_STAT("total_malloced", "%llu", (unsigned long long)mem_malloced);
       17:  404:    add_stats(NULL, 0, NULL, 0, c);
       17:  405:}
        -:  406:
      447:  407:static void *memory_allocate(size_t size) {
        -:  408:    void *ret;
        -:  409:
      447:  410:    if (mem_base == NULL) {
        -:  411:        /* We are not using a preallocated large memory chunk */
      447:  412:        ret = malloc(size);
        -:  413:    } else {
    #####:  414:        ret = mem_current;
        -:  415:
    #####:  416:        if (size > mem_avail) {
        -:  417:            return NULL;
        -:  418:        }
        -:  419:
        -:  420:        /* mem_current pointer _must_ be aligned!!! */
    #####:  421:        if (size % CHUNK_ALIGN_BYTES) {
    #####:  422:            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);
        -:  423:        }
        -:  424:
    #####:  425:        mem_current = ((char*)mem_current) + size;
    #####:  426:        if (size < mem_avail) {
    #####:  427:            mem_avail -= size;
        -:  428:        } else {
    #####:  429:            mem_avail = 0;
        -:  430:        }
        -:  431:    }
      447:  432:    mem_malloced += size;
        -:  433:
      447:  434:    return ret;
        -:  435:}
        -:  436:
    60739:  437:void *slabs_alloc(size_t size, unsigned int id, unsigned int *total_chunks,
        -:  438:        unsigned int flags) {
        -:  439:    void *ret;
        -:  440:
    60739:  441:    pthread_mutex_lock(&slabs_lock);
    60739:  442:    ret = do_slabs_alloc(size, id, total_chunks, flags);
    60739:  443:    pthread_mutex_unlock(&slabs_lock);
    60739:  444:    return ret;
        -:  445:}
        -:  446:
    37625:  447:void slabs_free(void *ptr, size_t size, unsigned int id) {
    37625:  448:    pthread_mutex_lock(&slabs_lock);
    37625:  449:    do_slabs_free(ptr, size, id);
    37625:  450:    pthread_mutex_unlock(&slabs_lock);
    37625:  451:}
        -:  452:
       17:  453:void slabs_stats(ADD_STAT add_stats, void *c) {
       17:  454:    pthread_mutex_lock(&slabs_lock);
       17:  455:    do_slabs_stats(add_stats, c);
       17:  456:    pthread_mutex_unlock(&slabs_lock);
       17:  457:}
        -:  458:
    #####:  459:void slabs_adjust_mem_requested(unsigned int id, size_t old, size_t ntotal)
        -:  460:{
    #####:  461:    pthread_mutex_lock(&slabs_lock);
        -:  462:    slabclass_t *p;
    #####:  463:    if (id < POWER_SMALLEST || id > power_largest) {
    #####:  464:        fprintf(stderr, "Internal error! Invalid slab class\n");
    #####:  465:        abort();
        -:  466:    }
        -:  467:
    #####:  468:    p = &slabclass[id];
    #####:  469:    p->requested = p->requested - old + ntotal;
    #####:  470:    pthread_mutex_unlock(&slabs_lock);
    #####:  471:}
        -:  472:
    51282:  473:unsigned int slabs_available_chunks(const unsigned int id, bool *mem_flag,
        -:  474:        unsigned int *total_chunks, unsigned int *chunks_perslab) {
        -:  475:    unsigned int ret;
        -:  476:    slabclass_t *p;
        -:  477:
    51282:  478:    pthread_mutex_lock(&slabs_lock);
    51282:  479:    p = &slabclass[id];
    51282:  480:    ret = p->sl_curr;
    51282:  481:    if (mem_flag != NULL)
    51282:  482:        *mem_flag = mem_limit_reached;
    51282:  483:    if (total_chunks != NULL)
    51282:  484:        *total_chunks = p->slabs * p->perslab;
    51282:  485:    if (chunks_perslab != NULL)
    51282:  486:        *chunks_perslab = p->perslab;
    51282:  487:    pthread_mutex_unlock(&slabs_lock);
    51282:  488:    return ret;
        -:  489:}
        -:  490:
        -:  491:static pthread_cond_t slab_rebalance_cond = PTHREAD_COND_INITIALIZER;
        -:  492:static volatile int do_run_slab_thread = 1;
        -:  493:static volatile int do_run_slab_rebalance_thread = 1;
        -:  494:
        -:  495:#define DEFAULT_SLAB_BULK_CHECK 1
        -:  496:int slab_bulk_check = DEFAULT_SLAB_BULK_CHECK;
        -:  497:
       88:  498:static int slab_rebalance_start(void) {
        -:  499:    slabclass_t *s_cls;
       88:  500:    int no_go = 0;
        -:  501:
       88:  502:    pthread_mutex_lock(&slabs_lock);
        -:  503:
      176:  504:    if (slab_rebal.s_clsid < POWER_SMALLEST ||
      176:  505:        slab_rebal.s_clsid > power_largest  ||
      176:  506:        slab_rebal.d_clsid < SLAB_GLOBAL_PAGE_POOL ||
       88:  507:        slab_rebal.d_clsid > power_largest  ||
        -:  508:        slab_rebal.s_clsid == slab_rebal.d_clsid)
    #####:  509:        no_go = -2;
        -:  510:
       88:  511:    s_cls = &slabclass[slab_rebal.s_clsid];
        -:  512:
       88:  513:    if (!grow_slab_list(slab_rebal.d_clsid)) {
    #####:  514:        no_go = -1;
        -:  515:    }
        -:  516:
       88:  517:    if (s_cls->slabs < 2)
    #####:  518:        no_go = -3;
        -:  519:
       88:  520:    if (no_go != 0) {
    #####:  521:        pthread_mutex_unlock(&slabs_lock);
    #####:  522:        return no_go; /* Should use a wrapper function... */
        -:  523:    }
        -:  524:
        -:  525:    /* Always kill the first available slab page as it is most likely to
        -:  526:     * contain the oldest items
        -:  527:     */
       88:  528:    slab_rebal.slab_start = s_cls->slab_list[0];
       88:  529:    slab_rebal.slab_end   = (char *)slab_rebal.slab_start +
       88:  530:        (s_cls->size * s_cls->perslab);
       88:  531:    slab_rebal.slab_pos   = slab_rebal.slab_start;
       88:  532:    slab_rebal.done       = 0;
        -:  533:
        -:  534:    /* Also tells do_item_get to search for items in this slab */
       88:  535:    slab_rebalance_signal = 2;
        -:  536:
       88:  537:    if (settings.verbose > 1) {
    #####:  538:        fprintf(stderr, "Started a slab rebalance\n");
        -:  539:    }
        -:  540:
       88:  541:    pthread_mutex_unlock(&slabs_lock);
        -:  542:
       88:  543:    STATS_LOCK();
       88:  544:    stats.slab_reassign_running = true;
       88:  545:    STATS_UNLOCK();
        -:  546:
       88:  547:    return 0;
        -:  548:}
        -:  549:
        -:  550:/* CALLED WITH slabs_lock HELD */
      156:  551:static void *slab_rebalance_alloc(const size_t size, unsigned int id) {
        -:  552:    slabclass_t *s_cls;
      156:  553:    s_cls = &slabclass[slab_rebal.s_clsid];
        -:  554:    int x;
      156:  555:    item *new_it = NULL;
        -:  556:
      212:  557:    for (x = 0; x < s_cls->perslab; x++) {
      212:  558:        new_it = do_slabs_alloc(size, id, NULL, SLABS_ALLOC_NO_NEWPAGE);
        -:  559:        /* check that memory isn't within the range to clear */
      212:  560:        if (new_it == NULL) {
        -:  561:            break;
        -:  562:        }
      132:  563:        if ((void *)new_it >= slab_rebal.slab_start
      132:  564:            && (void *)new_it < slab_rebal.slab_end) {
        -:  565:            /* Pulled something we intend to free. Mark it as freed since
        -:  566:             * we've already done the work of unlinking it from the freelist.
        -:  567:             */
       56:  568:            s_cls->requested -= size;
       56:  569:            new_it->refcount = 0;
       56:  570:            new_it->it_flags = ITEM_SLABBED|ITEM_FETCHED;
       56:  571:            new_it = NULL;
       56:  572:            slab_rebal.inline_reclaim++;
        -:  573:        } else {
        -:  574:            break;
        -:  575:        }
        -:  576:    }
      156:  577:    return new_it;
        -:  578:}
        -:  579:
        -:  580:enum move_status {
        -:  581:    MOVE_PASS=0, MOVE_FROM_SLAB, MOVE_FROM_LRU, MOVE_BUSY, MOVE_LOCKED
        -:  582:};
        -:  583:
        -:  584:/* refcount == 0 is safe since nobody can incr while item_lock is held.
        -:  585: * refcount != 0 is impossible since flags/etc can be modified in other
        -:  586: * threads. instead, note we found a busy one and bail. logic in do_item_get
        -:  587: * will prevent busy items from continuing to be busy
        -:  588: * NOTE: This is checking it_flags outside of an item lock. I believe this
        -:  589: * works since it_flags is 8 bits, and we're only ever comparing a single bit
        -:  590: * regardless. ITEM_SLABBED bit will always be correct since we're holding the
        -:  591: * lock which modifies that bit. ITEM_LINKED won't exist if we're between an
        -:  592: * item having ITEM_SLABBED removed, and the key hasn't been added to the item
        -:  593: * yet. The memory barrier from the slabs lock should order the key write and the
        -:  594: * flags to the item?
        -:  595: * If ITEM_LINKED did exist and was just removed, but we still see it, that's
        -:  596: * still safe since it will have a valid key, which we then lock, and then
        -:  597: * recheck everything.
        -:  598: * This may not be safe on all platforms; If not, slabs_alloc() will need to
        -:  599: * seed the item key while holding slabs_lock.
        -:  600: */
    10045:  601:static int slab_rebalance_move(void) {
        -:  602:    slabclass_t *s_cls;
        -:  603:    int x;
    10045:  604:    int was_busy = 0;
    10045:  605:    int refcount = 0;
        -:  606:    uint32_t hv;
        -:  607:    void *hold_lock;
    10045:  608:    enum move_status status = MOVE_PASS;
        -:  609:
    10045:  610:    pthread_mutex_lock(&slabs_lock);
        -:  611:
    10045:  612:    s_cls = &slabclass[slab_rebal.s_clsid];
        -:  613:
    20000:  614:    for (x = 0; x < slab_bulk_check; x++) {
    10045:  615:        hv = 0;
    10045:  616:        hold_lock = NULL;
    10045:  617:        item *it = slab_rebal.slab_pos;
    10045:  618:        status = MOVE_PASS;
        -:  619:        /* ITEM_FETCHED when ITEM_SLABBED is overloaded to mean we've cleared
        -:  620:         * the chunk for move. Only these two flags should exist.
        -:  621:         */
    10045:  622:        if (it->it_flags != (ITEM_SLABBED|ITEM_FETCHED)) {
        -:  623:            /* ITEM_SLABBED can only be added/removed under the slabs_lock */
     9803:  624:            if (it->it_flags & ITEM_SLABBED) {
        -:  625:                /* remove from slab freelist */
     9645:  626:                if (s_cls->slots == it) {
       80:  627:                    s_cls->slots = it->next;
        -:  628:                }
     9645:  629:                if (it->next) it->next->prev = it->prev;
     9645:  630:                if (it->prev) it->prev->next = it->next;
     9645:  631:                s_cls->sl_curr--;
     9645:  632:                status = MOVE_FROM_SLAB;
      158:  633:            } else if ((it->it_flags & ITEM_LINKED) != 0) {
        -:  634:                /* If it doesn't have ITEM_SLABBED, the item could be in any
        -:  635:                 * state on its way to being freed or written to. If no
        -:  636:                 * ITEM_SLABBED, but it's had ITEM_LINKED, it must be active
        -:  637:                 * and have the key written to it already.
        -:  638:                 */
      158:  639:                hv = hash(ITEM_key(it), it->nkey);
      158:  640:                if ((hold_lock = item_trylock(hv)) == NULL) {
        -:  641:                    status = MOVE_LOCKED;
        -:  642:                } else {
      156:  643:                    refcount = refcount_incr(&it->refcount);
      156:  644:                    if (refcount == 2) { /* item is linked but not busy */
        -:  645:                        /* Double check ITEM_LINKED flag here, since we're
        -:  646:                         * past a memory barrier from the mutex. */
      156:  647:                        if ((it->it_flags & ITEM_LINKED) != 0) {
        -:  648:                            status = MOVE_FROM_LRU;
        -:  649:                        } else {
        -:  650:                            /* refcount == 1 + !ITEM_LINKED means the item is being
        -:  651:                             * uploaded to, or was just unlinked but hasn't been freed
        -:  652:                             * yet. Let it bleed off on its own and try again later */
    #####:  653:                            status = MOVE_BUSY;
        -:  654:                        }
        -:  655:                    } else {
    #####:  656:                        if (settings.verbose > 2) {
    #####:  657:                            fprintf(stderr, "Slab reassign hit a busy item: refcount: %d (%d -> %d)\n",
    #####:  658:                                it->refcount, slab_rebal.s_clsid, slab_rebal.d_clsid);
        -:  659:                        }
        -:  660:                        status = MOVE_BUSY;
        -:  661:                    }
        -:  662:                    /* Item lock must be held while modifying refcount */
      156:  663:                    if (status == MOVE_BUSY) {
    #####:  664:                        refcount_decr(&it->refcount);
    #####:  665:                        item_trylock_unlock(hold_lock);
        -:  666:                    }
        -:  667:                }
        -:  668:            } else {
        -:  669:                /* See above comment. No ITEM_SLABBED or ITEM_LINKED. Mark
        -:  670:                 * busy and wait for item to complete its upload. */
        -:  671:                status = MOVE_BUSY;
        -:  672:            }
        -:  673:        }
        -:  674:
    10045:  675:        int save_item = 0;
    10045:  676:        item *new_it = NULL;
    10045:  677:        size_t ntotal = 0;
    10045:  678:        switch (status) {
        -:  679:            case MOVE_FROM_LRU:
        -:  680:                /* Lock order is LRU locks -> slabs_lock. unlink uses LRU lock.
        -:  681:                 * We only need to hold the slabs_lock while initially looking
        -:  682:                 * at an item, and at this point we have an exclusive refcount
        -:  683:                 * (2) + the item is locked. Drop slabs lock, drop item to
        -:  684:                 * refcount 1 (just our own, then fall through and wipe it
        -:  685:                 */
        -:  686:                /* Check if expired or flushed */
      156:  687:                ntotal = ITEM_ntotal(it);
        -:  688:                /* REQUIRES slabs_lock: CHECK FOR cls->sl_curr > 0 */
      156:  689:                if ((it->exptime != 0 && it->exptime < current_time)
      156:  690:                    || item_is_flushed(it)) {
        -:  691:                    /* TODO: maybe we only want to save if item is in HOT or
        -:  692:                     * WARM LRU?
        -:  693:                     */
        -:  694:                    save_item = 0;
      156:  695:                } else if ((new_it = slab_rebalance_alloc(ntotal, slab_rebal.s_clsid)) == NULL) {
       80:  696:                    save_item = 0;
       80:  697:                    slab_rebal.evictions_nomem++;
        -:  698:                } else {
        -:  699:                    save_item = 1;
        -:  700:                }
      156:  701:                pthread_mutex_unlock(&slabs_lock);
      156:  702:                if (save_item) {
        -:  703:                    /* if free memory, memcpy. clear prev/next/h_bucket */
        -:  704:                    memcpy(new_it, it, ntotal);
       76:  705:                    new_it->prev = 0;
       76:  706:                    new_it->next = 0;
       76:  707:                    new_it->h_next = 0;
        -:  708:                    /* These are definitely required. else fails assert */
       76:  709:                    new_it->it_flags &= ~ITEM_LINKED;
       76:  710:                    new_it->refcount = 0;
       76:  711:                    do_item_replace(it, new_it, hv);
       76:  712:                    slab_rebal.rescues++;
        -:  713:                } else {
       80:  714:                    do_item_unlink(it, hv);
        -:  715:                }
      156:  716:                item_trylock_unlock(hold_lock);
      156:  717:                pthread_mutex_lock(&slabs_lock);
        -:  718:                /* Always remove the ntotal, as we added it in during
        -:  719:                 * do_slabs_alloc() when copying the item.
        -:  720:                 */
      156:  721:                s_cls->requested -= ntotal;
        -:  722:            case MOVE_FROM_SLAB:
     9801:  723:                it->refcount = 0;
     9801:  724:                it->it_flags = ITEM_SLABBED|ITEM_FETCHED;
        -:  725:#ifdef DEBUG_SLAB_MOVER
        -:  726:                memcpy(ITEM_key(it), "deadbeef", 8);
        -:  727:#endif
     9801:  728:                break;
        -:  729:            case MOVE_BUSY:
        -:  730:            case MOVE_LOCKED:
        2:  731:                slab_rebal.busy_items++;
        2:  732:                was_busy++;
        2:  733:                break;
        -:  734:            case MOVE_PASS:
        -:  735:                break;
        -:  736:        }
        -:  737:
    10045:  738:        slab_rebal.slab_pos = (char *)slab_rebal.slab_pos + s_cls->size;
    10045:  739:        if (slab_rebal.slab_pos >= slab_rebal.slab_end)
        -:  740:            break;
        -:  741:    }
        -:  742:
    10045:  743:    if (slab_rebal.slab_pos >= slab_rebal.slab_end) {
        -:  744:        /* Some items were busy, start again from the top */
       90:  745:        if (slab_rebal.busy_items) {
        2:  746:            slab_rebal.slab_pos = slab_rebal.slab_start;
        2:  747:            STATS_LOCK();
        2:  748:            stats.slab_reassign_busy_items += slab_rebal.busy_items;
        2:  749:            STATS_UNLOCK();
        2:  750:            slab_rebal.busy_items = 0;
        -:  751:        } else {
       88:  752:            slab_rebal.done++;
        -:  753:        }
        -:  754:    }
        -:  755:
    10045:  756:    pthread_mutex_unlock(&slabs_lock);
        -:  757:
    10045:  758:    return was_busy;
        -:  759:}
        -:  760:
       88:  761:static void slab_rebalance_finish(void) {
        -:  762:    slabclass_t *s_cls;
        -:  763:    slabclass_t *d_cls;
        -:  764:    int x;
        -:  765:    uint32_t rescues;
        -:  766:    uint32_t evictions_nomem;
        -:  767:    uint32_t inline_reclaim;
        -:  768:
       88:  769:    pthread_mutex_lock(&slabs_lock);
        -:  770:
       88:  771:    s_cls = &slabclass[slab_rebal.s_clsid];
       88:  772:    d_cls = &slabclass[slab_rebal.d_clsid];
        -:  773:
        -:  774:#ifdef DEBUG_SLAB_MOVER
        -:  775:    /* If the algorithm is broken, live items can sneak in. */
        -:  776:    slab_rebal.slab_pos = slab_rebal.slab_start;
        -:  777:    while (1) {
        -:  778:        item *it = slab_rebal.slab_pos;
        -:  779:        assert(it->it_flags == (ITEM_SLABBED|ITEM_FETCHED));
        -:  780:        assert(memcmp(ITEM_key(it), "deadbeef", 8) == 0);
        -:  781:        it->it_flags = ITEM_SLABBED|ITEM_FETCHED;
        -:  782:        slab_rebal.slab_pos = (char *)slab_rebal.slab_pos + s_cls->size;
        -:  783:        if (slab_rebal.slab_pos >= slab_rebal.slab_end)
        -:  784:            break;
        -:  785:    }
        -:  786:#endif
        -:  787:
        -:  788:    /* At this point the stolen slab is completely clear.
        -:  789:     * We always kill the "first"/"oldest" slab page in the slab_list, so
        -:  790:     * shuffle the page list backwards and decrement.
        -:  791:     */
       88:  792:    s_cls->slabs--;
     2085:  793:    for (x = 0; x < s_cls->slabs; x++) {
     1997:  794:        s_cls->slab_list[x] = s_cls->slab_list[x+1];
        -:  795:    }
        -:  796:
       88:  797:    d_cls->slab_list[d_cls->slabs++] = slab_rebal.slab_start;
        -:  798:    /* Don't need to split the page into chunks if we're just storing it */
       88:  799:    if (slab_rebal.d_clsid > SLAB_GLOBAL_PAGE_POOL) {
       10:  800:        memset(slab_rebal.slab_start, 0, (size_t)settings.item_size_max);
       10:  801:        split_slab_page_into_freelist(slab_rebal.slab_start,
       10:  802:            slab_rebal.d_clsid);
        -:  803:    }
        -:  804:
       88:  805:    slab_rebal.done       = 0;
       88:  806:    slab_rebal.s_clsid    = 0;
       88:  807:    slab_rebal.d_clsid    = 0;
       88:  808:    slab_rebal.slab_start = NULL;
       88:  809:    slab_rebal.slab_end   = NULL;
       88:  810:    slab_rebal.slab_pos   = NULL;
       88:  811:    evictions_nomem    = slab_rebal.evictions_nomem;
       88:  812:    inline_reclaim = slab_rebal.inline_reclaim;
       88:  813:    rescues   = slab_rebal.rescues;
       88:  814:    slab_rebal.evictions_nomem    = 0;
       88:  815:    slab_rebal.inline_reclaim = 0;
       88:  816:    slab_rebal.rescues  = 0;
        -:  817:
       88:  818:    slab_rebalance_signal = 0;
        -:  819:
       88:  820:    pthread_mutex_unlock(&slabs_lock);
        -:  821:
       88:  822:    STATS_LOCK();
       88:  823:    stats.slab_reassign_running = false;
       88:  824:    stats.slabs_moved++;
       88:  825:    stats.slab_reassign_rescues += rescues;
       88:  826:    stats.slab_reassign_evictions_nomem += evictions_nomem;
       88:  827:    stats.slab_reassign_inline_reclaim += inline_reclaim;
       88:  828:    STATS_UNLOCK();
        -:  829:
       88:  830:    if (settings.verbose > 1) {
    #####:  831:        fprintf(stderr, "finished a slab move\n");
        -:  832:    }
       88:  833:}
        -:  834:
        -:  835:/* Slab mover thread.
        -:  836: * Sits waiting for a condition to jump off and shovel some memory about
        -:  837: */
        2:  838:static void *slab_rebalance_thread(void *arg) {
        2:  839:    int was_busy = 0;
        -:  840:    /* So we first pass into cond_wait with the mutex held */
        2:  841:    mutex_lock(&slabs_rebalance_lock);
        -:  842:
    10137:  843:    while (do_run_slab_rebalance_thread) {
    10135:  844:        if (slab_rebalance_signal == 1) {
       88:  845:            if (slab_rebalance_start() < 0) {
        -:  846:                /* Handle errors with more specifity as required. */
    #####:  847:                slab_rebalance_signal = 0;
        -:  848:            }
        -:  849:
        -:  850:            was_busy = 0;
    10047:  851:        } else if (slab_rebalance_signal && slab_rebal.slab_start != NULL) {
    10045:  852:            was_busy = slab_rebalance_move();
        -:  853:        }
        -:  854:
    10135:  855:        if (slab_rebal.done) {
       88:  856:            slab_rebalance_finish();
    10047:  857:        } else if (was_busy) {
        -:  858:            /* Stuck waiting for some items to unlock, so slow down a bit
        -:  859:             * to give them a chance to free up */
        2:  860:            usleep(50);
        -:  861:        }
        -:  862:
    10135:  863:        if (slab_rebalance_signal == 0) {
        -:  864:            /* always hold this lock while we're running */
       90:  865:            pthread_cond_wait(&slab_rebalance_cond, &slabs_rebalance_lock);
        -:  866:        }
        -:  867:    }
    #####:  868:    return NULL;
        -:  869:}
        -:  870:
        -:  871:/* Iterate at most once through the slab classes and pick a "random" source.
        -:  872: * I like this better than calling rand() since rand() is slow enough that we
        -:  873: * can just check all of the classes once instead.
        -:  874: */
     6666:  875:static int slabs_reassign_pick_any(int dst) {
        -:  876:    static int cur = POWER_SMALLEST - 1;
     6666:  877:    int tries = power_largest - POWER_SMALLEST + 1;
   286568:  878:    for (; tries > 0; tries--) {
   279910:  879:        cur++;
   279910:  880:        if (cur > power_largest)
     6664:  881:            cur = POWER_SMALLEST;
   279910:  882:        if (cur == dst)
     6665:  883:            continue;
   273245:  884:        if (slabclass[cur].slabs > 1) {
        -:  885:            return cur;
        -:  886:        }
        -:  887:    }
        -:  888:    return -1;
        -:  889:}
        -:  890:
     6746:  891:static enum reassign_result_type do_slabs_reassign(int src, int dst) {
     6746:  892:    if (slab_rebalance_signal != 0)
        -:  893:        return REASSIGN_RUNNING;
        -:  894:
     6746:  895:    if (src == dst)
        -:  896:        return REASSIGN_SRC_DST_SAME;
        -:  897:
        -:  898:    /* Special indicator to choose ourselves. */
     6746:  899:    if (src == -1) {
     6666:  900:        src = slabs_reassign_pick_any(dst);
        -:  901:        /* TODO: If we end up back at -1, return a new error type */
        -:  902:    }
        -:  903:
     6746:  904:    if (src < POWER_SMALLEST        || src > power_largest ||
       88:  905:        dst < SLAB_GLOBAL_PAGE_POOL || dst > power_largest)
        -:  906:        return REASSIGN_BADCLASS;
        -:  907:
       88:  908:    if (slabclass[src].slabs < 2)
        -:  909:        return REASSIGN_NOSPARE;
        -:  910:
       88:  911:    slab_rebal.s_clsid = src;
       88:  912:    slab_rebal.d_clsid = dst;
        -:  913:
       88:  914:    slab_rebalance_signal = 1;
       88:  915:    pthread_cond_signal(&slab_rebalance_cond);
        -:  916:
       88:  917:    return REASSIGN_OK;
        -:  918:}
        -:  919:
     6765:  920:enum reassign_result_type slabs_reassign(int src, int dst) {
        -:  921:    enum reassign_result_type ret;
     6765:  922:    if (pthread_mutex_trylock(&slabs_rebalance_lock) != 0) {
        -:  923:        return REASSIGN_RUNNING;
        -:  924:    }
     6746:  925:    ret = do_slabs_reassign(src, dst);
     6746:  926:    pthread_mutex_unlock(&slabs_rebalance_lock);
     6746:  927:    return ret;
        -:  928:}
        -:  929:
        -:  930:/* If we hold this lock, rebalancer can't wake up or move */
    #####:  931:void slabs_rebalancer_pause(void) {
    #####:  932:    pthread_mutex_lock(&slabs_rebalance_lock);
    #####:  933:}
        -:  934:
    #####:  935:void slabs_rebalancer_resume(void) {
    #####:  936:    pthread_mutex_unlock(&slabs_rebalance_lock);
    #####:  937:}
        -:  938:
        -:  939:static pthread_t rebalance_tid;
        -:  940:
        2:  941:int start_slab_maintenance_thread(void) {
        -:  942:    int ret;
        2:  943:    slab_rebalance_signal = 0;
        2:  944:    slab_rebal.slab_start = NULL;
        2:  945:    char *env = getenv("MEMCACHED_SLAB_BULK_CHECK");
        2:  946:    if (env != NULL) {
    #####:  947:        slab_bulk_check = atoi(env);
    #####:  948:        if (slab_bulk_check == 0) {
    #####:  949:            slab_bulk_check = DEFAULT_SLAB_BULK_CHECK;
        -:  950:        }
        -:  951:    }
        -:  952:
        2:  953:    if (pthread_cond_init(&slab_rebalance_cond, NULL) != 0) {
    #####:  954:        fprintf(stderr, "Can't intiialize rebalance condition\n");
    #####:  955:        return -1;
        -:  956:    }
        2:  957:    pthread_mutex_init(&slabs_rebalance_lock, NULL);
        -:  958:
        2:  959:    if ((ret = pthread_create(&rebalance_tid, NULL,
        -:  960:                              slab_rebalance_thread, NULL)) != 0) {
    #####:  961:        fprintf(stderr, "Can't create rebal thread: %s\n", strerror(ret));
    #####:  962:        return -1;
        -:  963:    }
        -:  964:    return 0;
        -:  965:}
        -:  966:
        -:  967:/* The maintenance thread is on a sleep/loop cycle, so it should join after a
        -:  968: * short wait */
    #####:  969:void stop_slab_maintenance_thread(void) {
    #####:  970:    mutex_lock(&slabs_rebalance_lock);
    #####:  971:    do_run_slab_thread = 0;
    #####:  972:    do_run_slab_rebalance_thread = 0;
    #####:  973:    pthread_cond_signal(&slab_rebalance_cond);
    #####:  974:    pthread_mutex_unlock(&slabs_rebalance_lock);
        -:  975:
        -:  976:    /* Wait for the maintenance thread to stop */
    #####:  977:    pthread_join(rebalance_tid, NULL);
    #####:  978:}
